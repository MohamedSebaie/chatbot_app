# Document ChatBot with vLLM

A powerful document chatbot that combines vLLM, FastAPI, and Streamlit to provide intelligent responses based on uploaded documents. The system supports both OpenAI GPT and Llama 3 models, featuring real-time streaming responses and conversation memory.

## Features
- ðŸ“‘ Document Processing: Upload and process PDF documents
- ðŸ’¬ Intelligent Chat: Context-aware responses using vLLM
- ðŸ” Document Search: Semantic search using FAISS
- ðŸš€ High Performance: Tensor parallelism support with vLLM
- ðŸŒ Modern Interface: Streamlit-based UI with real-time responses
- ðŸ“š Multi-document Support: Chat with multiple uploaded documents
- ðŸ”„ Context Retention: Maintains conversation context (Working on it)
- ðŸ“ˆ Source Citations: Provides references for responses

## Demo

https://github.com/MohamedSebaie/document-chatbot/demo.mp4

For the full demonstration video, check out our [detailed walkthrough](demo.mp4).

### Features Demonstrated
- ðŸ“ Document Upload & Processing
- ðŸ’¬ Interactive Chat Interface
- ðŸ” Context-Aware Responses
- ðŸ“š Source Citations
- âš¡ Real-time Processing
  
## System Requirements
- NVIDIA Driver >= 525.60.13
- CUDA Toolkit >= 12.1
- Python 3.10+
- 16GB+ RAM
- Ubuntu 22.04 or later

## Technical Stack
- **Backend**: FastAPI
- **Frontend**: Streamlit
- **Models**: OpenAI GPT, Llama 3
- **GPU parallelism**: vLLM
- **Vector Store**: FAISS
- **Document Processing**: PyPDF2, LangChain
- **Containerization**: Docker

## Project Structure
document-chatbot/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â””â”€â”€ routes.py # FastAPI routes
â”‚ â””â”€â”€ core/
â”‚ â”œâ”€â”€ config.py # Configuration settings
â”‚ â”œâ”€â”€ document_processor.py # PDF processing
â”‚ â””â”€â”€ exceptions.py # Custom exceptions
â”‚ â”œâ”€â”€ llm_manager.py # LLM integration
â”‚ â””â”€â”€ memory_manager.py # Conversation memory
â”œâ”€â”€ frontend/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ chat_interface.py # Streamlit chat interface
â”‚   â”œâ”€â”€ document_uploader.py # File upload handling
    â”œâ”€â”€ model_selector.py # File model selector
â”‚ â””â”€â”€ app.py # Streamlit application
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ start_services.py # Service orchestration
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ uploads/ # Document storage
â”‚ â””â”€â”€ vector_store/ # FAISS indexes
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ vllm_env.yaml # Conda environment
â””â”€â”€ README.md

## Installation Methods

### Method 1: Direct Installation (Recommended for Development)

1. Install NVIDIA Driver and CUDA Toolkit:
```bash
# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.1-530.30.02-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.1-530.30.02-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/

# Install CUDA
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-1

2. Install Miniconda:
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
source ~/.bashrc

3. Clone the repository:
git clone https://github.com/yourusername/document-chatbot.git
cd document-chatbot

4. Create and activate conda environment:
conda env create -f vllm_env.yaml
conda activate vllm_env

5. Create necessary directories:
mkdir -p data/uploads data/vector_store

6. Start the services:
python scripts/start_services.py
```
### Method 2: Docker Installation (Recommended for Production)
```bash
1. Install Docker:

# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Add your user to docker group
sudo usermod -aG docker $USER
newgrp docker

2. Install NVIDIA Container Toolkit:
# Add NVIDIA package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

# Install NVIDIA Docker support
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

3. Clone and build:
git clone https://github.com/yourusername/document-chatbot.git
cd document-chatbot

# Build and start services
docker-compose up --build

4. Stop services:
docker-compose down
```
## Usage
### Access the interfaces:
- Streamlit UI: http://localhost:8501
- FastAPI Docs: http://localhost:8080/docs
- vLLM API: http://localhost:8000/v1
### Upload Documents:
- Use the sidebar uploader
- Support for PDF files
- Wait for processing confirmation
### Chat Interface:
- Type questions in chat
- View source citations
- Clear chat history as needed

# License
MIT License - see LICENSE file for details

# Acknowledgments
- vLLM Team for the inference engine
- Hugging Face for model hosting
- FastAPI and Streamlit teams

# Contact
GitHub Issues: Project Issues
Email: mohamedsebaie1@gmail.com 
